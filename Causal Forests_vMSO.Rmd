---
title: "Random Forest_v1"
author: "Mariana Saldarriaga"
date: "12/30/2020"
output: html_document
---

# Install packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Generalized Randon Forest (cf. Athey and Wager)
install.packages("grf") 
install.packages("DiagrammeR")
```

```{r}
summary(ud_subset)
dim(ud_subset)
```


# Spliting the dataset into train and test set. Why? to not have overfiting problems. 
```{r}
## Make sure the data has no missing values (outcome and treatment to run causal forests!!!)
data_uganda_is.na <- data_uganda[!is.na(data_uganda$foll_index_income_gen_act), ]
data_uganda_na <- as_data_frame(data_uganda_is.na) 

# Complete cases for selected variables - OFER
data_uganda_na_o <- data_uganda %>%
  filter(complete.cases(foll_index_income_gen_act,
                        index_empowerment,
                        children, 
                        branchno,
                        age, 
                        rural,
                        income,
                        enroll_school,
                        vill_id,
                        study_hours)) # standardization for this one not working, but good one being used!

## Datacamp method (ratio 75:25); most of causal forest use this ratio but haven't found true argument to chose this ratio. 
# Get number of rows
N <- nrow(data_uganda_na_o)
# Calculate how many rows 75% of N should be and print it; we use round to get an integer
target <- round(N * 0.75)
# Create the vector of N uniform random variables: rv
rv <- runif(N)
# Use rv to create the training set: ud_train (75% of data) and ud_test (25% of data)
data_uganda_train <- data_uganda_na_o[rv < 0.75, ]
data_uganda_test <- data_uganda_na_o[rv >= 0.75, ]
# Use nrow() to examine ud_train and ud_test
nrow(data_uganda_train) # 2314
nrow(data_uganda_test) # 782 --> large enough?

## I think it is large enough the data set --> we don't need cross validation to split the data!

# Other method (ratio 70:30)
set.seed(400) # To always run same answer; or just put seed
train <- sample(nrow(data_uganda_na), 0.7*nrow(data_uganda_na), replace = FALSE)
data_uganda_train_v2 <- data_uganda_na[train,] #  Train is like the data we would collect in a randomized experiment 
data_uganda_valid_v2 <- data_uganda_na[-train,] # Test would be the future cases which we are trying to predict
summary(data_uganda_train_v2)
nrow(data_uganda_train)
summary(data_uganda_valid_v2)
nrow(data_uganda_test)
```

### NOTES: USE CAUSAL TREE FROM GRF PACKAGE (TUNE PARAMETERS ALL)

# Train a Causal Forests
```{r}
library(grf)
library(DiagrammeR)

# Random seed to reproduce results
set.seed

## INCOME GENERATING ACTIVITY - IGA
# Create outcome and inputs for the Causal Forests (important all numeric; including dummy code the factor variables)
Y_outcome <- as.matrix(data_uganda_train$foll_index_income_gen_act) # Vector outcome of interest

X_vars <- model.matrix(lm (Y_outcome ~ age + branchno, data = data_uganda_train)) # X is a matrix of the covariates which we are using to predict heterogeneity in treatment effect

## OFER --> TO DO: RUN without lm!!!??
X_vars <- model.matrix(lm (Y_outcome ~ children + branchno + age + rural + income + enroll_school + study_hours+ index_empowerment + enroll_school*study_hours  + children*age,
                            #dist_nearclub  
                           #age
                             #rich + 
                             #branchno + 
                             #value_assets + 
                             #rural+ 
                             #want_respect + 
                           #loan_brac +
                           #index_empowerment +
                             #enroll_school + 
                             #back_school + 
                             #att_earn_moneyfam +
                             #work_married+ 
                           #ablework_married+
                           #worry_job +
                           #income +
                           #back_school+
                           #partner +
                           #index_controlbody+
                           #index_aspiration+
                           #index_income_gen_act+
                             #who_decidehusband + 
                             #children + 
                             #life_skills + 
                             #study_hours + 
                             #sex_unwilling + 
                             #satisf_income +
                             #livelihood, 
                           data = data_uganda_train))

W_treatment <- as.matrix(data_uganda_train$treatment) # Treatment assignment

C_vars <- data_uganda_train$vill_id # Cluster used in paper

causalforests_IGA_train <- causal_forest(X = X_vars,
                                    Y = Y_outcome,
                                    W = W_treatment, 
                                    num.trees = 2000, #more trees result in better confident intervals
                                    clusters = C_vars, # paper clusters
                                    orthog.boosting = TRUE) 
                                    # but see again!!! Athey and Wager 2019; orthogonalization
# We don't mention here Y.hat and W.hat --> Null; software silently estimates Y.hat or W.hat via regression forests.

# Graph Causal Forests
plot(tree <- get_tree(causalforests_IGA_train, 1))

## Predict the test data
# Tell grf to include variance estimates, and then I assign the predictions (the estimated treatment effects) to the test data frame so that we can use these in subsequent analyses
preds_IGA <- predict(
  object = causalforests_IGA_train, 
  newdata = model.matrix(~ children + branchno + age + rural + income + enroll_school + index_empowerment + children*age, data = data_uganda_test), 
  estimate.variance = TRUE
)

data_uganda_test$preds_IGA <- preds_IGA$predictions[, 1]
```

# Variable importance and graph CATE
```{r}
# 
varimp_model_wlm <- variable_importance(causalforests_IGA_train ) # A matrix of importance measure, one row for each predictor variable. The column(s) are different importance measures.
selected_imp_variables <- which ( varimp_model_wlm > mean ( varimp_model_wlm ))
causalforests_IGA_train_vimpo <- causal_forest(X = X_vars[,selected_imp_variables],
                                                Y = Y_outcome,
                                                W = W_treatment,
                                                clusters = C_vars
                                                )
tau.hat = predict (cf )$ predictions
```



## For linear regression

# Train the model using test/train split
```{r}
# Datacamp method
summary(ud_train)

# Formula to express cty as a function of hwy: fmla and print it.
(fmla <- cty~hwy)

# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy 
mpg_model <- lm(fmla, mpg_train)

# Use summary() to examine the model
summary(mpg_model)
```

# Evaluate model using test/train split
```{r}
# predict cty from hwy for the training set
mpg_train$pred <- predict(mpg_model)

# predict cty from hwy for the test set
mpg_test$pred <- predict(mpg_model, newdata = mpg_test)

# Evaluate the rmse on both training and test data and print them
(rmse_train <- rmse(mpg_train$pred, mpg_train$cty))
(rmse_test <- rmse(mpg_test$pred, mpg_test$cty))


# Evaluate the r-squared on both training and test data.and print them
(rsq_train <- r_squared(mpg_train$pred, mpg_train$cty))
(rsq_test <- r_squared(mpg_test$pred, mpg_test$cty))

# Plot the predictions (on the x-axis) against the outcome (cty) on the test data
ggplot(mpg_test, aes(x = pred, y = cty)) + 
  geom_point() + 
  geom_abline()
```

