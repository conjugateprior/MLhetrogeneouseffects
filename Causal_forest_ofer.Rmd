---
title: "Causal Random Forest"
author: "Mariana Saldarriaga and Ofer Dotan"
date: "2021"
output: html_document
---

# Install packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Generalized Randon Forest (cf. Athey and Wager)
#install.packages("grf") 
```

```{r packages, fig.show='hide', message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#Load libraries
library(haven)
library(caret)
library(tidyverse) # remember includes dplyr
library(expss)
library(summarytools)
library(mice)

```

```{r}
# Load data
load("data_uganda.rda")
```


# Spliting the dataset into train and test set. Why? to not have overfiting problems. 

```{r}
## Datacamp method (ratio 75:25); most of causal forest use this ratio but haven't found true argument to chose this ratio. 
# Get number of rows
N <- nrow(data_uganda)
# Calculate how many rows 75% of N should be and print it; we use round to get an integer
target <- round(N * 0.75)
# Create the vector of N uniform random variables: rv
rv <- runif(N)
# Use rv to create the training set: ud_train (75% of data) and ud_test (25% of data)
data_uganda_train <- data_uganda[rv < 0.75, ]
data_uganda_test <- data_uganda[rv >= 0.75, ]
# Use nrow() to examine ud_train and ud_test
nrow(data_uganda_train)
nrow(data_uganda_test)

## I think it is large enough the data set --> we don't need cross validation to split the data!



# Other method (ratio 70:30)
set.seed(400) # To always run same answer; or just put seed
train <- sample(nrow(data_uganda), 0.7*nrow(data_uganda), replace = FALSE)
data_uganda_train_v2 <- data_uganda[train,] #  Train is like the data we would collect in a randomized experiment 
data_uganda_valid_v2 <- data_uganda[-train,] # Test would be the future cases which we are trying to predict
summary(data_uganda_train_v2)
nrow(data_uganda_train_v2)
summary(data_uganda_valid_v2)
nrow(data_uganda_valid_v2)

```


### NOTES: USE CAUSAL TREE FROM GRF PACKAGE (TUNE PARAMETERS ALL)

Generate X and Y matrixes
```{r}
library(grf)

# Random seed to reproduce results
set.seed

## INCOME GENERATING ACTIVITY - IGA
# Create outcome and inputs for the Causal Forests (important all numeric; including dummy code the factor variables)
Y_outcome <- as.matrix(data_uganda_train$foll_index_income_gen_act) # Vector outcome of interest
#attach outcome to train data
Y_aspiration <- as.matrix(data_uganda_train$foll_index_aspiration)

Y_empower <- as.matrix(data_uganda_train$foll_index_empowerment)


X_vars <- model.matrix(lm (Y_outcome ~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na + 
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                           data = data_uganda_train)) # X is a matrix of the covariates which we are using to predict heterogeneity in treatment effect

X_baseline <- model.matrix(lm(Y_outcome ~ children + children_na +
                                branchno + branchno_na +
                                age + age_na +
                                rural + rural_na +
                                income + income_na + 
                                enroll_school + enroll_school_na +
                                study_hours+ study_hours_na + 
                                index_empowerment + index_empowerment_na +
                                att_earn_moneyfam + att_earn_moneyfam_na +
                                z_Entrep_total + z_Entrep_total_na +
                                z_Expenditure_totDF + z_Expenditure_totDF_na +
                                value_assets + value_assets_na +
                                loan_brac + loan_brac_na +
                                ablework_married + ablework_married_na +
                                M_marrywhen + M_marrywhen_na +
                                who_decidehusband + who_decidehusband_na +
                                work_married + work_married_na +
                                want_respect + want_respect_na +
                                worry_job + worry_job_na +
                                satisf_income + satisf_income_na +
                                selfempl + selfempl_na +
                                hsworked_year_empl + hsworked_year_empl_na +
                                hsworked_year_self + hsworked_year_self_na +
                                life_skills + life_skills_na +
                                livelihood + livelihood_na +
                                enroll_school*study_hours + 
                                enroll_school_na*study_hours_na +
                                children*age +
                                children_na*age_na,
                              data = data_uganda_train))

#cluster
C_vars <- data_uganda_train$vill_id

# Treatment assignment
W_treatment <- as.matrix(data_uganda_train$treatment) 
```


Estimating treatment effects with causal forests. Throughout this note, we follow
editorial guidelines for the special issue of Observational Studies, and denote treatment assignment
by Z. However, the grf interface has different conventions, and treatment assignment is denoted by
W rather than Z (e.g., the function causal_forest actually has an argument W.hat, not Z.hat).
In order to get these code snippets to run in grf, all the \Z" need to be replaced with \W".

We train the Y.forest and Z.forest using default settings, as their predictions
are simply used as inputs to the causal forest and default parameter choices often perform
reasonably well with random forests.

```{r}
Y.forest = regression_forest(X_baseline, Y_outcome, clusters = C_vars)
Y.hat = predict(Y.forest)$predictions

W.forest = regression_forest(X_baseline, 
                             W_treatment, 
                             clusters = C_vars)
W.hat = predict(W.forest)$predictions


cf.raw = causal_forest (X_baseline, 
                        Y_outcome, 
                        W_treatment,
                        Y.hat = Y.hat, 
                        W.hat = W.hat,
                        clusters = C_vars)

#test set
X.test <- model.matrix(lm(foll_index_income_gen_act ~ children + children_na +
                                branchno + branchno_na +
                                age + age_na +
                                rural + rural_na +
                                income + income_na + 
                                enroll_school + enroll_school_na +
                                study_hours+ study_hours_na + 
                                index_empowerment + index_empowerment_na +
                                att_earn_moneyfam + att_earn_moneyfam_na +
                                z_Entrep_total + z_Entrep_total_na +
                                z_Expenditure_totDF + z_Expenditure_totDF_na +
                                value_assets + value_assets_na +
                                loan_brac + loan_brac_na +
                                ablework_married + ablework_married_na +
                                M_marrywhen + M_marrywhen_na +
                                who_decidehusband + who_decidehusband_na +
                                work_married + work_married_na +
                                want_respect + want_respect_na +
                                worry_job + worry_job_na +
                                satisf_income + satisf_income_na +
                                selfempl + selfempl_na +
                                hsworked_year_empl + hsworked_year_empl_na +
                                hsworked_year_self + hsworked_year_self_na +
                                life_skills + life_skills_na +
                                livelihood + livelihood_na +
                                enroll_school*study_hours + 
                                enroll_school_na*study_hours_na +
                                children*age +
                                children_na*age_na,
                              data = data_uganda_test))
#predict on test set
pred.cf <- predict(cf.raw, X.test, data=data_uganda_test, estimate.variance = TRUE)
```


```{r}
# Estimate treatment effects for the test sample.
####doesnt work yet
plot(X.test[, 1], pred.cf$predictions, ylim = range(pred.cf$predictions, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2)



# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(cf.raw, target.sample = "all")

#average treatment effect for the raw model
ATE = average_treatment_effect(cf.raw)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))

#accuracy check for raw model
accuracy <- postResample(pred = preds_cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy

#after training a raw model with all baseline variables, it is recommended to train additional model only with important variable
##check for variable importance
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp)) #select only important variables and train another causal forest using only important variables
cf = causal_forest(X_baseline[,selected.idx], 
                      Y_outcome, W_treatment, 
                      Y.hat = Y.hat, 
                      W.hat = W.hat, 
                      num.trees = 3000,
                      clusters = C_vars,
                      tune.parameters = "all")


tau.hat = predict(cf)
#plot frequency of predictions
hist(tau.hat$predictions)

#average treatment effect for the model with important variables
ATE = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))

#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis. Computes the best linear fit of the target estimand using the forest prediction (on held-out data) as well as the mean forest prediction as the sole two regressors. A coefficient of 1 for 'mean.forest.prediction' suggests that the mean forest prediction is correct, whereas a coefficient of 1 for 'differential.forest.prediction' additionally suggests that the forest has captured heterogeneity in the underlying signal. The p-value of the 'differential.forest.prediction' coefficient also acts as an omnibus test for the presence of heterogeneity: If the coefficient is significantly greater than 0, then we can reject the null of no heterogeneity.

test_calibration(cf)

#Finds the optimal ridge penalty for local linear causal prediction.
#A list of lambdas tried, corresponding errors, and optimal ridge penalty lambda.
tune_ll_causal_forest(cf,
                      linear.correction.variables = NULL,
                      ll.weight.penalty = FALSE,
                      num.threads = NULL,
                      lambda.path = NULL
                      )
# Compare villages with high and low estimated CATEs
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect)
ate.low = average_treatment_effect(cf, subset = !high_effect)
paste("95% CI for difference in ATE:",
      round(ate.high[1] - ate.low[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))


#####predict test set on only important variables
imp.variables.test <- data_uganda_test %>% select() #need to check how to select only important variables to check run the new model cf
imp.variables.test
```

Causal forest of basic model from paper
```{r}

#train causal forest
cf.basic <- causal_forest(X = X_vars,
                                    Y = Y_outcome,
                                    W = W_treatment, 
                                    Y.hat = Y.hat, 
                                    W.hat = W.hat, 
                                    num.trees = 3000, #more trees result in better confident intervals
                                    clusters = C_vars,
                                    orthog.boosting = TRUE,
                                    tune.parameters = "all")# paper clusters
                               
                                    # but see again!!! Athey and Wager 2019; orthogonalization
##check for variable importance
varimp.basic = variable_importance(cf.basic)
selected.idx.basic = which(varimp.basic > mean(varimp.basic)) #select only important variables and train another causal forest using only important variables
selected.idx.basic


## Predict the test data
# Tell grf to include variance estimates, and then I assign the predictions (the estimated treatment effects) to the test data frame so that we can use these in subsequent analyses
preds_IGA <- predict(
  object = cf.basic, 
  newdata = model.matrix(~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na + 
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                         data= data_uganda_test), 
  estimate.variance = TRUE
)

```


```{r}
library(DiagrammeR)
# Graph Causal Forests
plot(tree <- get_tree(cf, 1))
```



# Evaluate model using test/train split
```{r}
#install.packages("Metrics")
library(Metrics)

accuracy <- postResample(pred = preds_IGA$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy


#absolute error rate = 9% of the predictions are completely wrong at the moment
ae <- ae(data_uganda_test$foll_index_income_gen_act, preds_IGA$predictions)
ae


# Evaluate the rmse on both training and test data and print them 
#The root mean squared error (RMSE) is the average prediction error (square root of mean squared error).
#https://bookdown.org/mpfoley1973/data-sci/model-validation.html
rmse_train <- rmse(cf.basic$predictions, data_uganda_train$foll_index_income_gen_act)
rmse_train

rmse_test <- rmse(preds_IGA$predictions, data_uganda_test$foll_index_income_gen_act)
rmse_test

#mse

```

BART Model for heterogeneity estimation
```{r}
#install.packages("bartMachine")
library(bartMachine)

Bart_model_IGA <- bartMachine(X = X_baseline,
                              Y = Y_outcome,
                              Xy = data_uganda_train,
                              num_trees = 3000,
                              k = 2,
                              use_missing_data = TRUE)
                      





```

