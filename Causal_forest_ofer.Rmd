---
title: "Causal Random Forest"
author: "Mariana Saldarriaga and Ofer Dotan"
date: "2021"
output: html_document
---

# Install packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Generalized Randon Forest (cf. Athey and Wager)
#install.packages("grf") 
```

```{r packages, fig.show='hide', message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#Load libraries
library(grf)
library(haven)
library(caret)
library(tidyverse) # remember includes dplyr
library(expss)
library(summarytools)

```

```{r}
# Load data
load("data_uganda.rda")
```


# Spliting the dataset into train and test set. Why? to not have overfiting problems. 

```{r}
## Datacamp method (ratio 75:25); most of causal forest use this ratio but haven't found true argument to chose this ratio. 
# Get number of rows
N <- nrow(data_uganda)
# Calculate how many rows 75% of N should be and print it; we use round to get an integer
target <- round(N * 0.75)
# Create the vector of N uniform random variables: rv
rv <- runif(N)
# Use rv to create the training set: ud_train (75% of data) and ud_test (25% of data)
data_uganda_train <- data_uganda[rv < 0.75, ]
data_uganda_test <- data_uganda[rv >= 0.75, ]
# Use nrow() to examine ud_train and ud_test
nrow(data_uganda_train)
nrow(data_uganda_test)


# Another method (ratio 70:30)
set.seed(400) # To always run same answer; or just put seed
train <- sample(nrow(data_uganda), 0.7*nrow(data_uganda), replace = FALSE)
data_uganda_train_v2 <- data_uganda[train,] #  Train is like the data we would collect in a randomized experiment 
data_uganda_valid_v2 <- data_uganda[-train,] # Test would be the future cases which we are trying to predict
summary(data_uganda_train_v2)
nrow(data_uganda_train_v2)
summary(data_uganda_valid_v2)
nrow(data_uganda_valid_v2)

```


### NOTES: USE CAUSAL TREE FROM GRF PACKAGE (TUNE PARAMETERS ALL)

Generate X and Y matrixes
```{r, include=FALSE}
# Random seed to reproduce results
set.seed

## INCOME GENERATING ACTIVITY - IGA
# Create outcome and inputs for the Causal Forests (important all numeric; including dummy code the factor variables)
Y_outcome <- as.matrix(data_uganda_train$foll_index_income_gen_act) # Vector outcome of interest
#attach outcome to train data

#Baseline variables from paper
X_vars <- model.matrix(lm (Y_outcome ~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na + 
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                           data = data_uganda_train)) # X is a matrix of the covariates which we are using to predict heterogeneity in treatment effect

#most baseline variable that exist in data
X_baseline <- model.matrix(lm(Y_outcome ~ children + children_na +
                                branchno + branchno_na +
                                age + age_na +
                                rural + rural_na +
                                income + income_na + 
                                enroll_school + enroll_school_na +
                                study_hours+ study_hours_na + 
                                index_empowerment + index_empowerment_na +
                                att_earn_moneyfam + att_earn_moneyfam_na +
                                z_Entrep_total + z_Entrep_total_na +
                                z_Expenditure_totDF + z_Expenditure_totDF_na +
                                value_assets + value_assets_na +
                                loan_brac + loan_brac_na +
                                ablework_married + ablework_married_na +
                                M_marrywhen + M_marrywhen_na +
                                who_decidehusband + who_decidehusband_na +
                                work_married + work_married_na +
                                want_respect + want_respect_na +
                                worry_job + worry_job_na +
                                satisf_income + satisf_income_na +
                                selfempl + selfempl_na +
                                hsworked_year_empl + hsworked_year_empl_na +
                                hsworked_year_self + hsworked_year_self_na +
                                life_skills + life_skills_na +
                                livelihood + livelihood_na +
                                enroll_school*study_hours + 
                                enroll_school_na*study_hours_na +
                                children*age +
                                children_na*age_na,
                              data = data_uganda_train))

#cluster
C_vars <- data_uganda_train$vill_id

# Treatment assignment
W_treatment <- as.matrix(data_uganda_train$treatment) 
```


Estimating treatment effects with causal forests. Throughout this note, we follow
editorial guidelines for the special issue of Observational Studies, and denote treatment assignment
by Z. However, the grf interface has different conventions, and treatment assignment is denoted by
W rather than Z (e.g., the function causal_forest actually has an argument W.hat, not Z.hat).
In order to get these code snippets to run in grf, all the \Z" need to be replaced with \W".

We train the Y.forest and Z.forest using default settings, as their predictions
are simply used as inputs to the causal forest and default parameter choices often perform
reasonably well with random forests.

```{r  message=FALSE, warning=FALSE, include=FALSE}
Y.forest = regression_forest(X_baseline, Y_outcome, clusters = C_vars)
Y.hat = predict(Y.forest)$predictions

W.forest = regression_forest(X_baseline, 
                             W_treatment, 
                             clusters = C_vars)
W.hat = predict(W.forest)$predictions


cf.raw = causal_forest (X_baseline, 
                        Y_outcome, 
                        W_treatment,
                        num.trees = 6000,
                        Y.hat = Y.hat, 
                        W.hat = W.hat,
                        clusters = C_vars)


#test set
X.test <- model.matrix(lm(foll_index_income_gen_act ~ children + children_na +
                                branchno + branchno_na +
                                age + age_na +
                                rural + rural_na +
                                income + income_na + 
                                enroll_school + enroll_school_na +
                                study_hours+ study_hours_na + 
                                index_empowerment + index_empowerment_na +
                                att_earn_moneyfam + att_earn_moneyfam_na +
                                z_Entrep_total + z_Entrep_total_na +
                                z_Expenditure_totDF + z_Expenditure_totDF_na +
                                value_assets + value_assets_na +
                                loan_brac + loan_brac_na +
                                ablework_married + ablework_married_na +
                                M_marrywhen + M_marrywhen_na +
                                who_decidehusband + who_decidehusband_na +
                                work_married + work_married_na +
                                want_respect + want_respect_na +
                                worry_job + worry_job_na +
                                satisf_income + satisf_income_na +
                                selfempl + selfempl_na +
                                hsworked_year_empl + hsworked_year_empl_na +
                                hsworked_year_self + hsworked_year_self_na +
                                life_skills + life_skills_na +
                                livelihood + livelihood_na +
                                enroll_school*study_hours + 
                                enroll_school_na*study_hours_na +
                                children*age +
                                children_na*age_na,
                              data = data_uganda_test))
#predict on test set
pred.cf <- predict(cf.raw, X.test, num.trees = 6000, data=data_uganda_test, estimate.variance = TRUE)
```

In order for conditional average treatment effects to be properly identified, a datasetâ€™s propensity scores must be bounded away from 0 and 1. A simple way to validate this assumption is to calculate the propensity scores by regressing the treatment assignments W against X, and examining the out-of-bag predictions. Concretely, you can perform the following steps:
If there is strong overlap, the histogram will be concentrated away from 0 and 1. If the data is instead concentrated at the extremes, the overlap assumption likely does not hold.

```{r}
propensity.forest = regression_forest(X_baseline, W_treatment)
W.hat.p = predict(propensity.forest)$predictions
hist(W.hat.p, xlab = "propensity score")
```


```{r}
# Estimate treatment effects for the test sample.
####doesn't work yet
plot(X.test[, 1], pred.cf$predictions, ylim = range(pred.cf$predictions, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2)


# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(cf.raw, target.sample = "all")

#average treatment effect for the raw model
ATE = average_treatment_effect(cf.raw)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))

#accuracy check for raw model
accuracy <- postResample(pred = pred.cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy
```


```{r}
#after training a raw model with all baseline variables, it is recommended to train additional model only with important variable
##check for variable importance
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp)) #select only important variables and train another causal forest using only important variables
#train a causal forest only with selected important variables.
cf = causal_forest(X_baseline[,selected.idx], 
                      Y_outcome, W_treatment, 
                      Y.hat = Y.hat, 
                      W.hat = W.hat, 
                      num.trees = 100000,
                      clusters = C_vars,
                      tune.parameters = "all")


tau.hat = predict(cf)

#plot frequency of predictions
hist(tau.hat$predictions)

#predict test set
pred.cf <- predict(cf, X.test[,selected.idx], 
                   num.trees = 6000, 
                   data=data_uganda_test, 
                   estimate.variance = TRUE)

#plot frequency of predictions
hist(pred.cf$predictions)
```


```{r}
#average treatment effect for the model with important variables
ATE = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))
```

accuracy for developed model with selected important variables
```{r}
accuracy <- postResample(pred = pred.cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy
```

#check overlap assumption for selected index
#assumption holds when choosing only selected index
```{r}
propensity.forest.selected.inx = regression_forest(X_baseline[,selected.idx], W_treatment)
W.hat.p = predict(propensity.forest.selected.inx)$predictions
hist(W.hat.p, xlab = "propensity score")
```

```{r}
#standard error predictions
standard.error = sqrt(pred.cf$variance.estimates)

hist(standard.error)
```

```{r}
#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis. Computes the best linear fit of the target estimand using the forest prediction (on held-out data) as well as the mean forest prediction as the sole two regressors. A coefficient of 1 for 'mean.forest.prediction' suggests that the mean forest prediction is correct, whereas a coefficient of 1 for 'differential.forest.prediction' additionally suggests that the forest has captured heterogeneity in the underlying signal. The p-value of the 'differential.forest.prediction' coefficient also acts as an omnibus test for the presence of heterogeneity: If the coefficient is significantly greater than 0, then we can reject the null of no heterogeneity.

test_calibration(cf)

#Finds the optimal ridge penalty for local linear causal prediction.
#A list of lambdas tried, corresponding errors, and optimal ridge penalty lambda.
tune_ll_causal_forest(cf,
                      linear.correction.variables = NULL,
                      ll.weight.penalty = FALSE,
                      num.threads = NULL,
                      lambda.path = NULL
                      )
# Compare villages with high and low estimated CATEs
high_effect = tau.hat$predictions > median(tau.hat$predictions)
ate.high = average_treatment_effect(cf, subset = high_effect)
ate.low = average_treatment_effect(cf, subset = !high_effect)
paste("95% CI for difference in ATE:",
      round(ate.high[1] - ate.low[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))


```

Causal forest 
```{r}
#train causal forest
cf.basic <- causal_forest(X = X_vars,
                                    Y = Y_outcome,
                                    W = W_treatment, 
                                    Y.hat = Y.hat, 
                                    W.hat = W.hat, 
                                    num.trees = 10000, #more trees result in better confident intervals
                                    clusters = C_vars,
                                    orthog.boosting = TRUE,
                                    tune.parameters = "all")# paper clusters
                               
                                    # but see again!!! Athey and Wager 2019; orthogonalization
##check for variable importance
varimp.basic = variable_importance(cf.basic)
selected.idx.basic = which(varimp.basic > mean(varimp.basic)) #select only important variables and train another causal forest using only important variables
selected.idx.basic


## Predict the test data
# Tell grf to include variance estimates, and then I assign the predictions (the estimated treatment effects) to the test data frame so that we can use these in subsequent analyses
preds_IGA <- predict(
  object = cf.basic, 
  newdata = model.matrix(~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na + 
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                         data= data_uganda_test), 
  estimate.variance = TRUE
)
```


```{r}
#standard error predictions
standard.error = sqrt(preds_IGA$variance.estimates)

hist(standard.error)
```



```{r}
library(DiagrammeR)
# Graph Causal Forests
plot(tree <- get_tree(cf, 3))
tree$nodes

```



# Evaluate model using test/train split
```{r}
#install.packages("Metrics")
library(Metrics)

accuracy <- postResample(pred = preds_IGAt$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy


#absolute error rate = 9% of the predictions are completely wrong at the moment
ae <- ae(data_uganda_test$foll_index_income_gen_act, pred.cf$predictions)
ae


# Evaluate the rmse on both training and test data and print them 
#The root mean squared error (RMSE) is the average prediction error (square root of mean squared error).
#https://bookdown.org/mpfoley1973/data-sci/model-validation.html
rmse_train <- rmse(cf$predictions, data_uganda_train$foll_index_income_gen_act)
rmse_train

rmse_test <- rmse(pred.cf$predictions, data_uganda_test$foll_index_income_gen_act)
rmse_test

```


Run specific analysis to check heterogeneity in important variables
```{r}

#
# formal test for vill id
#
#tau.hat - predicting the causal forest object


vill.score = tau.hat + W_treatment/ cf$W.hat *
  (Y_outcome - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W_treatment) / (1 - cf$W.hat) * (Y_outcome - cf$Y.hat + cf$W.hat * tau.hat)

#from athey's code -- still trying to understand how to translate it to our data!!!!
school.score = t(school.mat) %*% dr.score / school.size

school.X1 = t(school.mat) %*% X$X1 / school.size
high.X1 = school.X1 > median(school.X1)
t.test(school.score[high.X1], school.score[!high.X1])

school.X2 = (t(school.mat) %*% X$X2) / school.size
high.X2 = school.X2 > median(school.X2)
t.test(school.score[high.X2], school.score[!high.X2])

school.X2.levels = cut(school.X2,
  breaks = c(-Inf, quantile(school.X2, c(1/3, 2/3)), Inf))
summary(aov(school.score ~ school.X2.levels))

#
# formal test for S3
#

school.score.XS3.high = t(school.mat) %*% (dr.score * (X$S3 >= 6)) /
  t(school.mat) %*% (X$S3 >= 6)
school.score.XS3.low = t(school.mat) %*% (dr.score * (X$S3 < 6)) /
  t(school.mat) %*% (X$S3 < 6)

plot(school.score.XS3.low, school.score.XS3.high)
t.test(school.score.XS3.high - school.score.XS3.low)

#
# Look at school-wise heterogeneity
#

pdf("school_hist.pdf")
pardef = par(mar = c(5, 4, 4, 2) + 0.5, cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
hist(school.score, xlab = "School Treatment Effect Estimate", main = "")
dev.off()

```

