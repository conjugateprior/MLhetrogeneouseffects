---
title: "Causal Random Forest"
author: "Ofer Dotan and Mariana Saldarriaga"
date: "2021"
output: html_document
---

# Install packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Generalized Randon Forest (cf. Athey and Wager)
#install.packages("grf") 
```
#Load libraries
```{r packages, fig.show='hide', message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#Load libraries
library(grf)
library(haven)
library(caret)
library(tidyverse) # remember includes dplyr
library(expss)
```

#Load data
```{r}
# Load data
load("data_uganda.rda")
```

#Split the dataset into train and test set. Why? to not have overfiting problems. 
```{r}
## Datacamp method (ratio 75:25); most of causal forest use this ratio but haven't found true argument to chose this ratio. 
# Get number of rows
N <- nrow(data_uganda)
# Calculate how many rows 75% of N should be and print it; we use round to get an integer
target <- round(N * 0.75)
# Create the vector of N uniform random variables: rv
rv <- runif(N)
# Use rv to create the training set: ud_train (75% of data) and ud_test (25% of data)
data_uganda_train <- data_uganda[rv < 0.75, ]
data_uganda_test <- data_uganda[rv >= 0.75, ]
# Use nrow() to examine ud_train and ud_test
nrow(data_uganda_train)
nrow(data_uganda_test)

```

#Generate X, Y, W (treatment), C(clusters) matrixes
```{r, include=FALSE}
# Random seed to reproduce results
set.seed(1236)

## INCOME GENERATING ACTIVITY - IGA
# Create outcome and inputs for the Causal Forests (important all numeric; including dummy code the factor variables)
Y_outcome <- as.matrix(data_uganda_train$foll_index_income_gen_act) # Vector outcome of interest
#attach outcome to train data

#Baseline variables from paper

X_vars <- model.matrix(lm (Y_outcome ~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na+
                             partner + partner_na+
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                           data = data_uganda_train)) # X is a matrix of the covariates which we are using to predict heterogeneity in treatment effect


#cluster
C_vars <- data_uganda_train$vill_id

# Treatment assignment
W_treatment <- as.matrix(data_uganda_train$treatment) 
```

#Estimate treatment effects with causal forests. 
train the Y.forest and Z.forest with default settings (later inputs to the causal forest)
```{r  message=FALSE, warning=FALSE, include=FALSE}
Y.forest = regression_forest(X_vars, 
                             Y_outcome, 
                             clusters = C_vars, 
                             tune.parameters = "all")

Y.hat = predict(Y.forest)$predictions

W.forest = regression_forest(X_vars, 
                             W_treatment, 
                             clusters = C_vars,
                             tune.parameters = "all")

W.hat = predict(W.forest)$predictions
```

#Train raw causal forest (no important variable selection)
```{r  message=FALSE, warning=FALSE, include=FALSE}
cf.raw = causal_forest (X_vars, 
                        Y_outcome, 
                        W_treatment,
                        num.trees = 6000,
                        Y.hat = Y.hat, 
                        W.hat = W.hat,
                        tune.parameters = "all",
                        clusters = C_vars)


#test set
X.test <- model.matrix( ~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na+
                             partner + partner_na+
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                              data = data_uganda_test)
#predict on test set
pred.cf.raw <- predict(cf.raw, X.test, num.trees = 6000, data=data_uganda_test, estimate.variance = TRUE)
```

#Validating the overlap assumption.
Estimate the data's propensity scores to ensure scores are bounded away from 0 and 1
In order for conditional average treatment effects to be properly identified
```{r}
propensity.forest = regression_forest(X_vars, W_treatment)
W.hat.p = predict(propensity.forest)$predictions
hist(W.hat.p, xlab = "propensity score")
```

#Estimate the average treatment effect for the full sample
```{r}
# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(cf.raw, target.sample = "all")
```

#Estimate the average treatment effect for the raw cf
```{r}
#average treatment effect for the raw model
ATE = average_treatment_effect(cf.raw)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))

```

#Find important variables
```{r}
##check for variable importance
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp>0.2)) #select only important variables and train another causal forest using only important variables
```

#Train a second causal forest using only important variables index
```{r}
#train a causal forest only with selected important variables.
cf = causal_forest(X_vars[,selected.idx], 
                      Y_outcome, W_treatment, 
                      Y.hat = Y.hat, 
                      W.hat = W.hat, 
                      num.trees = 6000,
                      clusters = C_vars,
                      tune.parameters = "all")
```

#Predict the tau hat of the cf and plot a histogram
```{r}
#predict the causal forest (train set)
tau.hat = predict(cf)$predictions

#plot frequency of predictions
hist(tau.hat$predictions)
```

#Predict the test set and plot histogram of frequency of predictions
```{r}
#predict test set
pred.cf <- predict(cf, X.test[,selected.idx], 
                   num.trees = 6000, 
                   data=data_uganda_test, 
                   tune.parameters = "all",
                   estimate.variance = TRUE)

#plot frequency of predictions
hist((pred.cf)$predictions)  
```

#Estimate ATE for cf model with important variables
```{r}
#average treatment effect for the model with important variables
ATE.cf = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE.cf[1], 3),
      "+/-", round(qnorm(0.975) * ATE.cf[2], 3))
```

#Estimate accuracy for model with selected important variables
```{r}
accuracy <- postResample(pred = pred.cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy
```

#Check overlap assumption for selected index
#assumption holds when choosing only selected index
```{r}
propensity.forest.selected.inx = regression_forest(X_vars[,selected.idx], W_treatment)
W.hat.p = predict(propensity.forest.selected.inx)$predictions
hist(W.hat.p, xlab = "propensity score")
```

#Estimate standard errors for test data and plot them
```{r}
#standard error predictions
standard.error = sqrt(pred.cf$variance.estimates)

hist(standard.error)
```

#Alternative method to estimate standard errors test data
```{r}
model_sigma <- sqrt(predict(cf, X.test[, selected.idx], estimate.variance = TRUE)$variance.estimates)
#model_sigma
```

#Estimate CATE and plot estimations
```{r}
new_data_cates <- as.data.frame(X.test[,-1])
new_data_cates$pred_est <- c(pred.cf$predictions)
new_data_cates$pred_var <- c(model_sigma)
new_data_cates$pred_est_lb <- new_data_cates$pred_est - 1.96 * new_data_cates$pred_var
new_data_cates$pred_est_ub <- new_data_cates$pred_est + 1.96 * new_data_cates$pred_var

# For each factor, get the average at each level
# Get results for every level of every variable by aggregating up
cates <- lapply(names(new_data_cates[, 1:4]), function(x) {
  tmp <- new_data_cates %>% 
    group_by_(x) %>% 
    transmute(
      variable = x,
      ate = round(mean(pred_est) * 100, 2),
      ate_lb = round(mean(pred_est_lb) * 100, 2),
      ate_ub = round(mean(pred_est_ub) * 100, 2)
      )%>% 
    unique() %>% 
    as.data.frame()
  tmp <- tmp[, c(2, 1, 3, 4, 5)]
  names(tmp)[2] <- "level"
  tmp
})


cates <- do.call(rbind, cates) %>% 
  mutate_if(is.character, as.factor)

cates.new.df <- as.data.frame(cates.new)


# visualize these
plot.cates <- ggplot(cates, aes(x = level, y = ate, colour = variable)) + 
  geom_point() +
  geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
  geom_hline(yintercept = 0, linetype = 3) +
  theme_light() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    strip.text.y = element_text(colour = "black"),
    strip.background = element_rect(colour = NA, fill = NA),
    legend.position = "none"
    ) + 
  facet_grid(variable ~ ., scales = "free_y") +
  coord_flip()


plot.cates  

```

#Estimate the overlap assumption to check ifcovariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms
```{r}
w.cf <- cf$W.hat
  
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / w.cf, 1 / (1 - tau.hat)))
# on train or test??
r <- 23
plot.df <- reshape(plot.df, varying = list(1:r), v.names = "X", direction = "long",
                   times = factor(paste0("X.", 1:r), levels = paste0("X.", 1:r)))

ggplot(plot.df, aes(x = X_vars, weight = IPW, fill = W_treatment)) + 
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  facet_wrap( ~ time, ncol = 2)

ggplot(plot.df, aes(x = X_vars, weight = IPW, fill = W_treatment)) 

dim(X_vars)
```

#Best linear predictor analysis to understand if the model detects heterogeneity 
```{r}
test_calibration(cf)
```

#Compare villages with high or low ATE estimations
```{r}
high_effect = tau.hat > median(tau.hat)
ate.high = average_treatment_effect(cf, subset = high_effect)
ate.low = average_treatment_effect(cf, subset = !high_effect)
paste("95% CI for difference in ATE:",
      round(ate.high[1] - ate.low[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))
```

#Estimate model bias
```{r}
p <- mean(W_treatment)
Y.hat.0 <- cf$Y.hat - w.cf * tau.hat
Y.hat.1 <- cf$Y.hat + (1 - w.cf) * tau.hat

bias <- (w.cf - p) * (p * (Y.hat.0 - mean(Y.hat.0))  + (1 - p) * (Y.hat.1 - mean(Y.hat.1)))

hist(bias / sd(Y_outcome))
```

#Plot a tree
```{r}
library(DiagrammeR)
# Graph Causal Forests
plot(tree <- get_tree(cf, 3))
tree$nodes
```


#Evaluate model using test/train split
```{r}
#install.packages("Metrics")
library(Metrics)

#accuracy check for raw model
accuracy <- postResample(pred = pred.cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy


# Evaluate the rmse on both training and test data and print them 
rmse_train <- rmse(cf$predictions, data_uganda_train$foll_index_income_gen_act)
rmse_train

rmse_test <- rmse(pred$predictions, data_uganda_test$foll_index_income_gen_act)
rmse_test

```


#Run specific analysis to check heterogeneity in important variables
```{r}
#test heterogeneous in rural and urban

# formal test for specific variables

vill.score = tau.hat + W_treatment / cf$W.hat *
  (Y_outcome - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W_treatment) / (1 - cf$W.hat) * (Y_outcome - cf$Y.hat + cf$W.hat * tau.hat)

vill.mat = model.matrix(~ vill_id + 0, data = data_uganda)
library(base)
vill.score = t(vill.mat) %*% vill.score

school.X1 = t(school.mat) %*% X$X1 / school.size
high.X1 = school.X1 > median(school.X1)
t.test(school.score[high.X1], school.score[!high.X1])

school.X2 = (t(school.mat) %*% X$X2) / school.size
high.X2 = school.X2 > median(school.X2)
t.test(school.score[high.X2], school.score[!high.X2])

school.X2.levels = cut(school.X2,
  breaks = c(-Inf, quantile(school.X2, c(1/3, 2/3)), Inf))
summary(aov(school.score ~ school.X2.levels))
```

