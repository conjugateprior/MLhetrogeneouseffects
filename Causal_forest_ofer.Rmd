---
title: "Causal Random Forest"
author: "Mariana Saldarriaga and Ofer Dotan"
date: "2021"
output: html_document
---

# Install packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Generalized Randon Forest (cf. Athey and Wager)
#install.packages("grf") 
```

```{r packages, fig.show='hide', message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#Load libraries
library(grf)
library(haven)
library(caret)
library(tidyverse) # remember includes dplyr
library(expss)
library(summarytools)

```

```{r}
# Load data
load("data_uganda.rda")
```


# Spliting the dataset into train and test set. Why? to not have overfiting problems. 

```{r}
## Datacamp method (ratio 75:25); most of causal forest use this ratio but haven't found true argument to chose this ratio. 
# Get number of rows
N <- nrow(data_uganda)
# Calculate how many rows 75% of N should be and print it; we use round to get an integer
target <- round(N * 0.75)
# Create the vector of N uniform random variables: rv
rv <- runif(N)
# Use rv to create the training set: ud_train (75% of data) and ud_test (25% of data)
data_uganda_train <- data_uganda[rv < 0.75, ]
data_uganda_test <- data_uganda[rv >= 0.75, ]
# Use nrow() to examine ud_train and ud_test
nrow(data_uganda_train)
nrow(data_uganda_test)


# Another method (ratio 70:30)
set.seed(400) # To always run same answer; or just put seed
train <- sample(nrow(data_uganda), 0.7*nrow(data_uganda), replace = FALSE)
data_uganda_train_v2 <- data_uganda[train,] #  Train is like the data we would collect in a randomized experiment 
data_uganda_valid_v2 <- data_uganda[-train,] # Test would be the future cases which we are trying to predict
summary(data_uganda_train_v2)
nrow(data_uganda_train_v2)
summary(data_uganda_valid_v2)
nrow(data_uganda_valid_v2)

```


### NOTES: USE CAUSAL TREE FROM GRF PACKAGE (TUNE PARAMETERS ALL)

Generate X and Y matrixes
```{r, include=FALSE}
# Random seed to reproduce results
set.seed(123)

## INCOME GENERATING ACTIVITY - IGA
# Create outcome and inputs for the Causal Forests (important all numeric; including dummy code the factor variables)
Y_outcome <- as.matrix(data_uganda_train$foll_index_income_gen_act) # Vector outcome of interest
#attach outcome to train data

#Baseline variables from paper

X_vars <- model.matrix(lm (Y_outcome ~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na+
                             partner + partner_na+
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                           data = data_uganda_train)) # X is a matrix of the covariates which we are using to predict heterogeneity in treatment effect


#cluster
C_vars <- data_uganda_train$vill_id

# Treatment assignment
W_treatment <- as.matrix(data_uganda_train$treatment) 
```


Estimating treatment effects with causal forests. Throughout this note, we follow
editorial guidelines for the special issue of Observational Studies, and denote treatment assignment
by Z. However, the grf interface has different conventions, and treatment assignment is denoted by
W rather than Z (e.g., the function causal_forest actually has an argument W.hat, not Z.hat).
In order to get these code snippets to run in grf, all the \Z" need to be replaced with \W".

We train the Y.forest and Z.forest using default settings, as their predictions
are simply used as inputs to the causal forest and default parameter choices often perform
reasonably well with random forests.

```{r  message=FALSE, warning=FALSE, include=FALSE}
Y.forest = regression_forest(X_vars, 
                             Y_outcome, 
                             clusters = C_vars, 
                             tune.parameters = "all")

Y.hat = predict(Y.forest)$predictions

W.forest = regression_forest(X_vars, 
                             W_treatment, 
                             clusters = C_vars,
                             tune.parameters = "all")

W.hat = predict(W.forest)$predictions


cf.raw = causal_forest (X_vars, 
                        Y_outcome, 
                        W_treatment,
                        num.trees = 6000,
                        Y.hat = Y.hat, 
                        W.hat = W.hat,
                        tune.parameters = "all",
                        clusters = C_vars)


#test set
X.test <- model.matrix( ~ children + children_na +
                             branchno + branchno_na +
                             age + age_na +
                             rural + rural_na +
                             income + income_na+
                             partner + partner_na+
                             enroll_school + enroll_school_na +
                             study_hours+ study_hours_na + 
                             index_empowerment + index_empowerment_na +
                             enroll_school*study_hours + 
                             enroll_school_na*study_hours_na +
                             children*age +
                             children_na*age_na,
                              data = data_uganda_test)
#predict on test set
pred.cf.raw <- predict(cf.raw, X.test, num.trees = 6000, data=data_uganda_test, estimate.variance = TRUE)
```

In order for conditional average treatment effects to be properly identified, a datasetâ€™s propensity scores must be bounded away from 0 and 1. A simple way to validate this assumption is to calculate the propensity scores by regressing the treatment assignments W against X, and examining the out-of-bag predictions. Concretely, you can perform the following steps:
If there is strong overlap, the histogram will be concentrated away from 0 and 1. If the data is instead concentrated at the extremes, the overlap assumption likely does not hold.

```{r}
propensity.forest = regression_forest(X_vars, W_treatment)
W.hat.p = predict(propensity.forest)$predictions
hist(W.hat.p, xlab = "propensity score")
```


```{r}

# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(cf.raw, target.sample = "all")

#average treatment effect for the raw model
ATE = average_treatment_effect(cf.raw)
paste("95% CI for the ATE:", round(ATE[1], 3),
      "+/-", round(qnorm(0.975) * ATE[2], 3))

#accuracy check for raw model
accuracy <- postResample(pred = pred.cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy
```

after training a raw model with all baseline variables, it is recommended to train additional model only with important variable
```{r}
##check for variable importance
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp>0.2)) #select only important variables and train another causal forest using only important variables


#train a causal forest only with selected important variables.
cf = causal_forest(X_vars[,selected.idx], 
                      Y_outcome, W_treatment, 
                      Y.hat = Y.hat, 
                      W.hat = W.hat, 
                      num.trees = 6000,
                      clusters = C_vars,
                      tune.parameters = "all")

#predict the causal forest (train set)
tau.hat = predict(cf)

#plot frequency of predictions
hist(tau.hat$predictions)
```


```{r}
#predict test set
pred.cf <- predict(cf, X.test[,selected.idx], 
                   num.trees = 6000, 
                   data=data_uganda_test, 
                   tune.parameters = "all",
                   estimate.variance = TRUE)

#plot frequency of predictions
hist((pred.cf)$predictions)  
```


```{r}
#average treatment effect for the model with important variables
ATE.cf = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE.cf[1], 3),
      "+/-", round(qnorm(0.975) * ATE.cf[2], 3))
```

accuracy for developed model with selected important variables
```{r}
accuracy <- postResample(pred = pred.cf$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy
```

#check overlap assumption for selected index
#assumption holds when choosing only selected index
```{r}
propensity.forest.selected.inx = regression_forest(X_vars[,selected.idx], W_treatment)
W.hat.p = predict(propensity.forest.selected.inx)$predictions
hist(W.hat.p, xlab = "propensity score")
```

standard errors for test data
```{r}
#standard error predictions
standard.error = sqrt(pred.cf$variance.estimates)

hist(standard.error)
```

alternative method to estimate standard errors test data
```{r}
model_sigma <- sqrt(predict(cf, X.test[, selected.idx], estimate.variance = TRUE)$variance.estimates)
#model_sigma
```

estimating CATE
```{r}
new_data_cates <- as.data.frame(X.test[,-1])
new_data_cates$pred_est <- c(pred.cf$predictions)
new_data_cates$pred_var <- c(model_sigma)
new_data_cates$pred_est_lb <- new_data_cates$pred_est - 1.96 * new_data_cates$pred_var
new_data_cates$pred_est_ub <- new_data_cates$pred_est + 1.96 * new_data_cates$pred_var

# For each factor, get the average at each level
# Get results for every level of every variable by aggregating up
cates <- lapply(names(new_data_cates[, 1:4]), function(x) {
  tmp <- new_data_cates %>% 
    group_by_(x) %>% 
    transmute(
      variable = x,
      ate = round(mean(pred_est) * 100, 2),
      ate_lb = round(mean(pred_est_lb) * 100, 2),
      ate_ub = round(mean(pred_est_ub) * 100, 2)
    ) %>% 
    unique() %>% 
    as.data.frame()
  tmp <- tmp[, c(2, 1, 3, 4, 5)]
  names(tmp)[2] <- "level"
  tmp
})



cates <- do.call(rbind, cates) %>% 
  mutate_if(is.character, as.factor)

cates.new.df <- as.data.frame(cates.new)


# visualize these
ggplot(cates, aes(x = level, y = ate, colour = variable)) + 
  geom_point() +
  geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
  geom_hline(yintercept = 0, linetype = 3) +
  theme_light() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    strip.text.y = element_text(colour = "black"),
    strip.background = element_rect(colour = NA, fill = NA),
    legend.position = "none"
    ) + 
  facet_grid(variable ~ ., scales = "free_y") +
  coord_flip()


plot.cates  

```

estimate the overlap
```{r}
# One can also check that the covariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms of all samples, overlaid here for each feature (done with ggplot2 which supports weighted histograms).
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / W.hat, 1 / (1 - W.hat)))
# on train or test??

#plot.df <- reshape(plot.df, varying = list(1), v.names = "X_vars", direction = "long",
#                   times = factor(paste0("X.", 1:), levels = paste0("X_vars.", 1:p)))

ggplot(plot.df, aes(x = X_vars, weight = IPW, fill = W_treatment)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  facet_wrap( ~ time, ncol = 2)
```


```{r}
#
# Omnibus tests for heterogeneity
#

# Run best linear predictor analysis. Computes the best linear fit of the target estimand using the forest prediction (on held-out data) as well as the mean forest prediction as the sole two regressors. A coefficient of 1 for 'mean.forest.prediction' suggests that the mean forest prediction is correct, whereas a coefficient of 1 for 'differential.forest.prediction' additionally suggests that the forest has captured heterogeneity in the underlying signal. The p-value of the 'differential.forest.prediction' coefficient also acts as an omnibus test for the presence of heterogeneity: If the coefficient is significantly greater than 0, then we can reject the null of no heterogeneity.

test_calibration(cf)
```


```{r}

# Compare villages with high and low estimated CATEs
high_effect = tau.hat$predictions > median(tau.hat$predictions)
ate.high = average_treatment_effect(cf, subset = high_effect)
ate.low = average_treatment_effect(cf, subset = !high_effect)
paste("95% CI for difference in ATE:",
      round(ate.high[1] - ate.low[1], 3), "+/-",
      round(qnorm(0.975) * sqrt(ate.high[2]^2 + ate.low[2]^2), 3))


```


```{r}
library(DiagrammeR)
# Graph Causal Forests
plot(tree <- get_tree(cf, 3))
tree$nodes

```


# Evaluate model using test/train split
```{r}
#install.packages("Metrics")
library(Metrics)

accuracy <- postResample(pred = pred.cf.basic.imp$predictions, 
                         obs = data_uganda_test$foll_index_income_gen_act)
accuracy


#absolute error rate = 9% of the predictions are completely wrong at the moment
ae <- ae(data_uganda_test$foll_index_income_gen_act,
         pred.cf.basic.imp$predictions)
ae


# Evaluate the rmse on both training and test data and print them 
#The root mean squared error (RMSE) is the average prediction error (square root of mean squared error).
#https://bookdown.org/mpfoley1973/data-sci/model-validation.html
rmse_train <- rmse(cf$predictions, data_uganda_train$foll_index_income_gen_act)
rmse_train

rmse_test <- rmse(pred$predictions, data_uganda_test$foll_index_income_gen_act)
rmse_test

```


Run specific analysis to check heterogeneity in important variables

```{r}
#test heterogeneous in rural and urban

# formal test for specific variables

vill.score = tau.hat + W_treatment / cf$W.hat *
  (Y_outcome - cf$Y.hat - (1 - cf$W.hat) * tau.hat) -
  (1 - W_treatment) / (1 - cf$W.hat) * (Y_outcome - cf$Y.hat + cf$W.hat * tau.hat)

vill.mat = model.matrix(~ vill_id + 0, data = data_uganda)
library(base)
vill.score = t(vill.mat) %*% vill.score

school.X1 = t(school.mat) %*% X$X1 / school.size
high.X1 = school.X1 > median(school.X1)
t.test(school.score[high.X1], school.score[!high.X1])

school.X2 = (t(school.mat) %*% X$X2) / school.size
high.X2 = school.X2 > median(school.X2)
t.test(school.score[high.X2], school.score[!high.X2])

school.X2.levels = cut(school.X2,
  breaks = c(-Inf, quantile(school.X2, c(1/3, 2/3)), Inf))
summary(aov(school.score ~ school.X2.levels))
```

