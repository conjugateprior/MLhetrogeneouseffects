num.trees = 6000,
data=data_uganda_test,
estimate.variance = TRUE)
#plot frequency of predictions
hist((pred.cf)$predictions)
#after training a raw model with all baseline variables, it is recommended to train additional model only with important variable
##check for variable importance
varimp = variable_importance(cf.raw)
View(varimp)
selected.idx = which(varimp > mean(varimp>0.2)) #select only important variables and train another causal forest using only important variables
#train a causal forest only with selected important variables.
cf = causal_forest(X_vars[,selected.idx],
Y_outcome, W_treatment,
Y.hat = Y.hat,
W.hat = W.hat,
num.trees = 6000,
clusters = C_vars,
tune.parameters = "all")
#predict the causal forest (train set)
tau.hat = predict(cf)$predection
#predict the causal forest (train set)
tau.hat = predict(cf)$predections
#predict the causal forest (train set)
tau.hat = predict(cf)
#predict test set
pred.cf <- predict(cf, X.test[,selected.idx],
num.trees = 6000,
data=data_uganda_test,
tune.parameters = "all",
estimate.variance = TRUE)
#plot frequency of predictions
hist((pred.cf)$predictions)
#standard error predictions
standard.error = sqrt(pred.cf$variance.estimates)
hist(standard.error)
new_data_cates <- as.data.frame(X.test[,-1])
new_data_cates$pred_est <- c(pred.cf$predictions)
new_data_cates$pred_var <- c(model_sigma)
new_data_cates$pred_est_lb <- new_data_cates$pred_est - 1.96 * new_data_cates$pred_var
new_data_cates$pred_est_ub <- new_data_cates$pred_est + 1.96 * new_data_cates$pred_var
# For each factor, get the average at each level
# Get results for every level of every variable by aggregating up
library(ggplot2)
cates <- lapply(names(new_data_cates[, 1:4]), function(x) {
tmp <- new_data_cates %>%
group_by_(x) %>%
transmute(
variable = x,
ate = round(mean(pred_est) * 100, 2),
ate_lb = round(mean(pred_est_lb) * 100, 2),
ate_ub = round(mean(pred_est_ub) * 100, 2)
) %>%
unique() %>%
as.data.frame()
tmp <- tmp[, c(2, 1, 3, 4, 5)]
names(tmp)[2] <- "level"
tmp
})
cates.new <- do.call(rbind, cates) %>%
mutate_if(is.character, as.factor)
cates.new.df <- as.data.frame(cates.new)
cates.plot <- ggproto("cates.plot", cates, required_aes = aes(x = level, y = cates$ate, color = variable))
# visualize these
library(ggplot2)
plot.cates <- ggplot(cates.new, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
cates <- lapply(names(new_data_cates[, 1:4]), function(x) {
tmp <- new_data_cates %>%
group_by_(x) %>%
transmute(
variable = x,
ate = round(mean(pred_est) * 100, 2),
ate_lb = round(mean(pred_est_lb) * 100, 2),
ate_ub = round(mean(pred_est_ub) * 100, 2)
) %>%
unique() %>%
as.data.frame()
tmp <- tmp[, c(2, 1, 3, 4, 5)]
names(tmp)[2] <- "level"
tmp
})
cates <- do.call(rbind, cates) %>%
mutate_if(is.character, as.factor)
plot.cates <- ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
test_calibration(cf)
plot.cates <- ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
rlang::last_error()
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
#panel.grid.major.x = element_blank(),
#panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
#panel.grid.major.x = element_blank(),
#panel.grid.minor.x = element_blank(),
#strip.text.y = element_text(colour = "black"),
#strip.background = element_rect(colour = NA, fill = NA),
#legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
inherit.aes = FALSE,
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
inherit.aes = FALSE +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
ggproto() +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
ggplot(cates) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
ggproto(aes(x = level, y = ate, color = variable)) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
# One can also check that the covariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms of all samples, overlaid here for each feature (done with ggplot2 which supports weighted histograms).
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / e.hat, 1 / (1 - e.hat)))
# One can also check that the covariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms of all samples, overlaid here for each feature (done with ggplot2 which supports weighted histograms).
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / w.hat, 1 / (1 - w.hat)))
# One can also check that the covariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms of all samples, overlaid here for each feature (done with ggplot2 which supports weighted histograms).
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / W.hat, 1 / (1 - W.hat)))
# on train or test??
plot.df <- reshape(plot.df, varying = list(1:p), v.names = "X_vars_paper", direction = "long",
times = factor(paste0("X.", 1:p), levels = paste0("X_vars_paper.", 1:p)))
# on train or test??
plot.df <- reshape(plot.df, varying = list(1:p), v.names = "X_vars", direction = "long",
times = factor(paste0("X.", 1:p), levels = paste0("X_vars.", 1:p)))
propensity.forest = regression_forest(X_vars, W_treatment)
W.hat.p = predict(propensity.forest)$predictions
hist(W.hat.p, xlab = "propensity score")
# One can also check that the covariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms of all samples, overlaid here for each feature (done with ggplot2 which supports weighted histograms).
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / W.hat, 1 / (1 - W.hat)))
plot.df <- reshape(plot.df, varying = list(1:), v.names = "X_vars", direction = "long",
plot.df <- reshape(plot.df, varying = NULL, v.names = "X_vars", direction = "long",
times = factor(paste0("X_vars."), levels = paste0("X_vars.")))
ggplot(plot.df, aes(x = X_vars, weight = IPW, fill = W_treatment)) +
geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
facet_wrap( ~ time, ncol = 2)
# One can also check that the covariates are balanced across the treated and control group by plotting the inverse-propensity weighted histograms of all samples, overlaid here for each feature (done with ggplot2 which supports weighted histograms).
plot.df <- data.frame(X_vars, W = as.factor(W_treatment), IPW = ifelse(W_treatment == 1, 1 / W.hat, 1 / (1 - W.hat)))
ggplot(plot.df, aes(x = X_vars, weight = IPW, fill = W_treatment)) +
geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
facet_wrap( ~ time, ncol = 2)
# Evaluate the rmse on both training and test data and print them
#The root mean squared error (RMSE) is the average prediction error (square root of mean squared error).
#https://bookdown.org/mpfoley1973/data-sci/model-validation.html
rmse_train <- rmse(cf$predictions, data_uganda_train$foll_index_income_gen_act)
View(cates)
knitr::opts_chunk$set(echo = TRUE)
# Generalized Randon Forest (cf. Athey and Wager)
#install.packages("grf")
#Load libraries
library(grf)
library(haven)
library(caret)
library(tidyverse) # remember includes dplyr
library(expss)
library(summarytools)
# Load data
load("data_uganda.rda")
## Datacamp method (ratio 75:25); most of causal forest use this ratio but haven't found true argument to chose this ratio.
# Get number of rows
N <- nrow(data_uganda)
# Calculate how many rows 75% of N should be and print it; we use round to get an integer
target <- round(N * 0.75)
# Create the vector of N uniform random variables: rv
rv <- runif(N)
# Use rv to create the training set: ud_train (75% of data) and ud_test (25% of data)
data_uganda_train <- data_uganda[rv < 0.75, ]
data_uganda_test <- data_uganda[rv >= 0.75, ]
# Use nrow() to examine ud_train and ud_test
nrow(data_uganda_train)
nrow(data_uganda_test)
# Another method (ratio 70:30)
set.seed(400) # To always run same answer; or just put seed
train <- sample(nrow(data_uganda), 0.7*nrow(data_uganda), replace = FALSE)
data_uganda_train_v2 <- data_uganda[train,] #  Train is like the data we would collect in a randomized experiment
data_uganda_valid_v2 <- data_uganda[-train,] # Test would be the future cases which we are trying to predict
summary(data_uganda_train_v2)
nrow(data_uganda_train_v2)
summary(data_uganda_valid_v2)
nrow(data_uganda_valid_v2)
# Random seed to reproduce results
set.seed
## INCOME GENERATING ACTIVITY - IGA
# Create outcome and inputs for the Causal Forests (important all numeric; including dummy code the factor variables)
Y_outcome <- as.matrix(data_uganda_train$foll_index_income_gen_act) # Vector outcome of interest
#attach outcome to train data
#Baseline variables from paper
X_vars <- model.matrix(lm (Y_outcome ~ children + children_na +
branchno + branchno_na +
age + age_na +
rural + rural_na +
income + income_na+
partner + partner_na+
enroll_school + enroll_school_na +
study_hours+ study_hours_na +
index_empowerment + index_empowerment_na +
enroll_school*study_hours +
enroll_school_na*study_hours_na +
children*age +
children_na*age_na,
data = data_uganda_train)) # X is a matrix of the covariates which we are using to predict heterogeneity in treatment effect
#cluster
C_vars <- data_uganda_train$vill_id
# Treatment assignment
W_treatment <- as.matrix(data_uganda_train$treatment)
Y.forest = regression_forest(X_vars,
Y_outcome,
clusters = C_vars,
tune.parameters = "all")
Y.hat = predict(Y.forest)$predictions
W.forest = regression_forest(X_vars,
W_treatment,
clusters = C_vars,
tune.parameters = "all")
W.hat = predict(W.forest)$predictions
cf.raw = causal_forest (X_vars,
Y_outcome,
W_treatment,
num.trees = 6000,
Y.hat = Y.hat,
W.hat = W.hat,
tune.parameters = "all",
clusters = C_vars)
#test set
X.test <- model.matrix( ~ children + children_na +
branchno + branchno_na +
age + age_na +
rural + rural_na +
income + income_na+
partner + partner_na+
enroll_school + enroll_school_na +
study_hours+ study_hours_na +
index_empowerment + index_empowerment_na +
enroll_school*study_hours +
enroll_school_na*study_hours_na +
children*age +
children_na*age_na,
data = data_uganda_test)
#predict on test set
pred.cf.raw <- predict(cf.raw, X.test, num.trees = 6000, data=data_uganda_test, estimate.variance = TRUE)
propensity.forest = regression_forest(X_vars, W_treatment)
W.hat.p = predict(propensity.forest)$predictions
hist(W.hat.p, xlab = "propensity score")
# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(cf.raw, target.sample = "all")
#average treatment effect for the raw model
ATE = average_treatment_effect(cf.raw)
paste("95% CI for the ATE:", round(ATE[1], 3),
"+/-", round(qnorm(0.975) * ATE[2], 3))
#accuracy check for raw model
accuracy <- postResample(pred = pred.cf$predictions,
obs = data_uganda_test$foll_index_income_gen_act)
accuracy
##check for variable importance
varimp = variable_importance(cf.raw)
selected.idx = which(varimp > mean(varimp>0.2)) #select only important variables and train another causal forest using only important variables
#train a causal forest only with selected important variables.
cf = causal_forest(X_vars[,selected.idx],
Y_outcome, W_treatment,
Y.hat = Y.hat,
W.hat = W.hat,
num.trees = 6000,
clusters = C_vars,
tune.parameters = "all")
#predict the causal forest (train set)
tau.hat = predict(cf)
#plot frequency of predictions
hist(tau.hat$predictions)
#predict test set
pred.cf <- predict(cf, X.test[,selected.idx],
num.trees = 6000,
data=data_uganda_test,
tune.parameters = "all",
estimate.variance = TRUE)
#plot frequency of predictions
hist((pred.cf)$predictions)
#average treatment effect for the model with important variables
ATE.cf = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE.cf[1], 3),
"+/-", round(qnorm(0.975) * ATE.cf[2], 3))
accuracy <- postResample(pred = pred.cf$predictions,
obs = data_uganda_test$foll_index_income_gen_act)
accuracy
propensity.forest.selected.inx = regression_forest(X_vars[,selected.idx], W_treatment)
W.hat.p = predict(propensity.forest.selected.inx)$predictions
hist(W.hat.p, xlab = "propensity score")
#standard error predictions
standard.error = sqrt(pred.cf$variance.estimates)
hist(standard.error)
model_sigma <- sqrt(predict(cf, X.test[, selected.idx], estimate.variance = TRUE)$variance.estimates)
model_sigma
new_data_cates <- as.data.frame(X.test[,-1])
new_data_cates$pred_est <- c(pred.cf$predictions)
new_data_cates$pred_var <- c(model_sigma)
new_data_cates$pred_est_lb <- new_data_cates$pred_est - 1.96 * new_data_cates$pred_var
new_data_cates$pred_est_ub <- new_data_cates$pred_est + 1.96 * new_data_cates$pred_var
# For each factor, get the average at each level
# Get results for every level of every variable by aggregating up
library(ggplot2)
cates <- lapply(names(new_data_cates[, 1:4]), function(x) {
tmp <- new_data_cates %>%
group_by_(x) %>%
transmute(
variable = x,
ate = round(mean(pred_est) * 100, 2),
ate_lb = round(mean(pred_est_lb) * 100, 2),
ate_ub = round(mean(pred_est_ub) * 100, 2)
) %>%
unique() %>%
as.data.frame()
tmp <- tmp[, c(2, 1, 3, 4, 5)]
names(tmp)[2] <- "level"
tmp
})
cates <- do.call(rbind, cates) %>%
mutate_if(is.character, as.factor)
cates.new.df <- as.data.frame(cates.new)
cates.plot <- ggproto("cates.plot", cates, required_aes = aes(x = level, y = cates$ate, color = variable))
new_data_cates <- as.data.frame(X.test[,-1])
new_data_cates$pred_est <- c(pred.cf$predictions)
new_data_cates$pred_var <- c(model_sigma)
new_data_cates$pred_est_lb <- new_data_cates$pred_est - 1.96 * new_data_cates$pred_var
new_data_cates$pred_est_ub <- new_data_cates$pred_est + 1.96 * new_data_cates$pred_var
# For each factor, get the average at each level
# Get results for every level of every variable by aggregating up
library(ggplot2)
cates <- lapply(names(new_data_cates[, 1:4]), function(x) {
tmp <- new_data_cates %>%
group_by_(x) %>%
transmute(
variable = x,
ate = round(mean(pred_est) * 100, 2),
ate_lb = round(mean(pred_est_lb) * 100, 2),
ate_ub = round(mean(pred_est_ub) * 100, 2)
) %>%
unique() %>%
as.data.frame()
tmp <- tmp[, c(2, 1, 3, 4, 5)]
names(tmp)[2] <- "level"
tmp
})
cates <- do.call(rbind, cates) %>%
mutate_if(is.character, as.factor)
cates.new.df <- as.data.frame(cates.new)
# visualize these
library(ggplot2)
ggplot(cates, aes(x = level, y = ate, color = variable)) +
theme_light() +
theme(
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
strip.text.y = element_text(colour = "black"),
strip.background = element_rect(colour = NA, fill = NA),
legend.position = "none"
) +
geom_point() +
geom_errorbar(aes(ymin = ate_lb, ymax = ate_ub), width = .2) +
geom_hline(yintercept = 0, linetype = 3) +
facet_grid(variable ~ ., scales = "free_y") +
coord_flip()
cates
dim(cates)
ls()
X.test
new_data_cates
str(new_data_cates)
str(new_data_cates[.1:4])
str(new_data_cates[,1:4])
cates
#average treatment effect for the model with important variables
ATE.cf = average_treatment_effect(cf)
paste("95% CI for the ATE:", round(ATE.cf[1], 3),
"+/-", round(qnorm(0.975) * ATE.cf[2], 3))
ggplot(cates, aes(x = level, y = ate, color = variable)) + geom_point()
str()
str()
str(cates)
ggplot(cates, aes(x = level, y = ate) + geom_point()
ggplot(cates, aes(x = level, y = ate)) + geom_point()
cates
ggplot(cates, aes(x = level, y = ate)) + geom_point()
str(cates)
ggplot(cates, aes(x = level, y = ate)) + geom_point(color = variable)
last_error()
rlang::last_error()
ggplot(mtcars, aes(wt,mpg)) + geom_point()
# visualize these
library(ggplot2)
ggplot(mtcars, aes(wt,mpg)) + geom_point()
mtcars
str(mtcars)
ggplot(mtcars, aes(wt,mpg)) + geom_point()
ggplot(plot.df, aes(x = X_vars, weight = IPW, fill = W_treatment)) +
geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
facet_wrap( ~ time, ncol = 2)
rm(list=ls())
